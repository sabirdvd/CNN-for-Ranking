please go back to original webpage 
http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
 Variable           explanation                                                            code                                                                                                   
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 sequence_length                                                                                                                                                 
                    The length of our sentences.                                                                                                                 
                    Remember that we padded all                                                                                                                  
                    our sentences to have the                                                                                                                    
                    same length (59 for our data set)                                                                                                            
                                                                                                                                                                 
                                                                                                                                                                 
                                                                                                                                                                 
 num_class          Number of classes in the output                                                                                                              
                                                          src_LANG[python]{                                                                                      
                    layer, two in our case positive       class TextCNN(object):                                                                                 
                    and negative                          def __init__(                                                                                          
                                                          self, sequence_length, num_classes, vocab_size,                                                        
                                                          embedding_size, filter_sizes, num_filters):                                                            
                                                          }                                                                                                      
                                                                                                                                                                 
                                                                                                                                                                 
 vocab_size         The size of our vocabulary.                                                                                                                  
                    This is need to define the size                                                                                                              
                    of our embedding layer, which                                                                                                                
                    will have shape                                                                                                                              
                    [vocabulary_size, embedding_size]                                                                                                            
                                                                                                                                                                 
                                                                                                                                                                 
 embedding_size     The dimensionality of our                                                                                                                    
                    embedding                                                                                                                                    
                                                                                                                                                                 
                                                                                                                                                                 
 filter_size        The number of words we want our                                                                                                              
                    convolutional filters to cover.                                                                                                              
                    We will have number_filters for                                                                                                              
                    each size specified here.                                                                                                                    
                                                                                                                                                                 
                    `For example' [3,4,5] mean that                                                                                                              
                    we will have filters that slide                                                                                                              
                    over 3,4 and 5 word respectively                                                                                                             
                    for a total of                                                                                                                               
                    *3 * numb_filters* filters                                                                                                                   
                                                                                                                                                                 
 number of filters  The number of filters per filter                                                                                                             
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 placeholders       tf.placeholder create a placeholder   sdsd # Placeholders for input, output and dropout                                                      
                    variable that we feed to the                                                                                                                 
                    network when we execute it at         self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name="input_x")                       
                    train ans test time.                  self.input_y = tf.placeholder(tf.float32, [None, num_classes], name="input_y")                         
                                                          self.dropout_keep_prob = tf.placeholder(tf.float32, name="dropout_keep_prob")                          
                    The second argument is the shape                                                                                                             
                    of the input tensor. *None* means                                                                                                            
                    that the length of the dimension                                                                                                             
                    could by anything. In our case,                                                                                                              
                    the first dimension is the                                                                                                                   
                    *batch size*, and using  *None*       batch size = None that allow the network to handle it arbitrarily size batch                           
                    allows the network to handle                                                                                                                 
                    arbitrarily sized batch.                                                                                                                     
                                                                                                                                                                 
                    The probability of keeping a                                                                                                                 
                    neuron in the *dropout layer* is                                                                                                             
                    also an input to the network                                                                                                                 
                    because we enable dropout only                                                                                                               
                    during training. *We disable it when                                                                                                         
                    evaluating the model*                                                                                                                        
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Embedding Layer                                          with tf.device('/cpu:0'), tf.name_scope("embedding"):                                                  
                                                          W = tf.Variable(                                                                                       
                                                                                                                                                                 
                                                          tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),                                            
 tf.device(cpu)     tf.device("/cpu:0") forces an                                                                                                                
                    operation to be executed on CPU.      name="W")                                                                                              
                    By default TF will try to put the                                                                                                            
                    operation on GPU but EM layers is     self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)                                          
                    not supported by GPU not yet                                                                                                                 
                                                          self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)                                 
 tf.name_scope      creates a new *Name Scope* with                                                                                                              
                    name "embedding". The scope adds                                                                                                             
                    all operation into a top-level                                                                                                               
                    node called "embedding" so that                                                                                                              
                    you get a nice hierarchy when                                                                                                                
                    visualizing you network in                                                                                                                   
                    TensorBoard.                                                                                                                                 
                                                                                                                                                                 
                                                                                                                                                                 
 W                  W is our embedding matrix that we                                                                                                            
                    learn during the training. We                                                                                                                
                    initialize it using random unifrom                                                                                                           
                    distribution.                                                                                                                                
                                                                                                                                                                 
 tf.embedding_                                                                                                                                                   
 lookup             tf.embadding_lookup create the                                                                                                               
                    actual embedding operation. The                                                                                                              
                    result of the embedding operation                                                                                                            
                    is a 3-dimensional tensor of shape>   3-D tensor of shape [None, sequence_length, embedding_size]                                            
                                                                                                                                                                 
 conv2d             TF conv2d operation expect 4-d                                                                                                               
                    tensor with dimension corresponding   4 -D [batch, width, height, channel]                                                                   
                    to batch, width, height and channel                                                                                                          
                    dimension, so we add it manually,                                                                                                            
                    leaving us with a layer of shape >    [None, sequence_length, embedding_size, 1]                                                             
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Conv and Pool                                                                                                                                                   
                                                                                                                                                                 
                    Now we're ready to build out conv     pooled_output = []                                                                                     
                    layer followed by max-pooling.        for i, filter_size in enumerate(filter_sizes):                                                         
                    *Remember* that we use filters of     with tf.name_scope("conv-maxpool-%s" % filter_size):                                                   
                    *diffrent sizes* , becaues each       *# Convolution Layer*                                                                                  
                    conv produces tensor of different     filter_shape = [filter_size, embedding_size, 1, num_filters]                                           
                    shapes we need to iterate through     W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name="W")                               
                    them. create a layer for each of      b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name="b")                                       
                    them and then *merge* the result      conv = tf.nn.conv2d(                                                                                   
                    into one big feature vector.          self.embedded_chars_expanded,                                                                          
                                                          W,                                                                                                     
                                                          strides=[1, 1, 1, 1],                                                                                  
                    Here, `W' is our filter matrix and    padding="VALID",                                                                                       
                    `h' is the result of applying the     name="conv")                                                                                           
                    nonlinearity to the convolution       *# Apply nonlinearity*                                                                                 
                    output. Each filter slides over the   h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")                                                   
                    whole embedding, but varies on how    # Max-pooling over the outputs                                                                         
                    many words it covers.                 pooled = tf.nn.max_pool(                                                                               
                                                          h,                                                                                                     
                    "VALID" padding mean that we slide    ksize=[1, sequence_length - filter_size + 1, 1, 1],                                                    
                    the filter over our sentence          strides=[1, 1, 1, 1],                                                                                  
                    without padding the edges,            padding='VALID',                                                                                       
                    preforming a a narrow conv that       name="pool")                                                                                           
                    give the output of shape:             pooled_outputs.append(pooled)                                                                          
                    [1, sequence_length - filter_size +                                                                                                          
                    1,1,1].                               *# Combine all the pooled features*                                                                    
                                                          num_filters_total = num_filters * len(filter_sizes)                                                    
                    Preforming max-pooling over the       self.h_pool = tf.concat(3, pooled_outputs)                                                             
                    output of a specific filter size      self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])                                    
                    leave us with tensor of shape:                                                                                                               
                    [batch_size,1,1, num_filters]                                                                                                                
                                                                                                                                                                 
                    This is essentially a *feature                                                                                                               
                    vector*, where the last dimension                                                                                                            
                    corresponds to out features                                                                                                                  
                                                                                                                                                                 
                    Once we have all the pooled output                                                                                                           
                    tensors from each filters size                                                                                                               
                    we combine them into one long                                                                                                                
                    feature vector vector of shape:                                                                                                              
                    [batch_size, num_filters_total]                                                                                                              
                                                                                                                                                                 
                    Using -1 in tf.reshape tells                                                                                                                 
                    tensorflow to flatten the dimension                                                                                                          
                    when possible.                                                                                                                               
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Dropout Layer      Dropout is perhaps most popular                                                                                                              
                    method to regularize CNN. The idea    # add dropout                                                                                          
                    behind dropout is simple, that                                                                                                               
                    dopout layer stochastically           with tf.name_scope("dropout"):                                                                         
                    " disable" a fraction of its          self.h.drop = tf.nn.dropout(self.h pool flat, self.dropout keep probe)                                 
                    neurons. This prevent neurons from                                                                                                           
                    co-adapting and force them to learn                                                                                                          
                    individually useful feature.                                                                                                                 
                                                                                                                                                                 
                    The fraction of neurons we keep                                                                                                              
                    enable is defined by the                                                                                                                     
                    `dropout_keep_prob' input to our                                                                                                             
                    network.We set this to something                                                                                                             
                    like 0.5 during training, and to                                                                                                             
                    1 (disable dropout) during                                                                                                                   
                    evaluation.                                                                                                                                  
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Score and                                                with tf.name_scope("output"):                                                                          
 Predictions        Using the feature vector from         W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name="W")           
                    max-pooling (with dropout applied)    b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name="b")                                       
                    we can generate predictions by        self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name="scores")                                        
                    doing a matrix multiplication         self.predictions = tf.argmax(self.scores, 1, name="predictions")                                       
                    and picking the class with highest                                                                                                           
                    score. We could also apple a                                                                                                                 
                    softmax function to convert raw                                                                                                              
                    score into normalized probabilities,                                                                                                         
                    but that wouldn't change our final                                                                                                           
                    predictions.                                                                                                                                 
                                                                                                                                                                 
                    Here, see the code,                                                                                                                          
                    tf.nn.xw_plus_b od a convenience                                                                                                             
                    wrapper to perform the Wx+b matrix                                                                                                           
                    multiplication/                                                                                                                              
                                                                                                                                                                 
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Loss and           using our score we can define the     # Calculate mean cross-entropy loss                                                                    
 Accuracy           loss function. The loss is a          with tf.name_scope("loss"):                                                                            
                    measurement of the error our          losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)                            
                    makes, and our goal is to minimize    self.loss = tf.reduce_                                                                                 
                    it.                                                                                                                                          
                    The standard loss function for                                                                                                               
                    categorization problem it the                                                                                                                
                    `cross-entroy loss'                                                                                                                          
                                                                                                                                                                 
                    Here, see the code, is a convenience  `tf.nn.softmax_cross_entropy_with_logits'                                                              
                    function that calculates the corss-                                                                                                          
 lo gits            entropy loss for each class, given                                                                                                           
                    our scores and the correct input                                                                                                             
                    labels.                                                                                                                                      
                                                                                                                                                                 
                    We then take the mean of the                                                                                                                 
                    losses. we could also use the sum,                                                                                                           
                    but that make it harder to compare                                                                                                           
                    the loss across different batch                                                                                                              
                    size and train/dev data                                                                                                                      
                                                                                                                                                                 
                    We also define an expression for      # Calculate Accuracy                                                                                   
                    the accuracy, which is a useful       with tf.name_scope("accuracy")                                                                         
                    quantity to keep track of             corret_predicitions = tf.equal(self.predictions, tf.argmax(self.input_y,1)                             
                    during training and testing           self.accuracy = tf.reduce mean(tf.cast(correct predictions, "float"), name="accuracy")                 
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Training                                                                                                                                                        
 procedure          Before we define the training                                                                                                                
                    procedure for our network we          with tf.Graph().as_default():                                                                          
                    need to understand how to use         session_conf = tf.ConfigProto(                                                                         
                    *Session* and *Grpahs* in TF.         allow_soft_placement=FLAGS.allow_soft_placement,                                                       
                                                          log_device_placement=FLAGS.log_device_placement)                                                       
                    Each program need one Graph. The      sess = tf.Session(config=session_conf)                                                                 
                    allow_soft_placement setting          with sess.as_default():                                                                                
                    allows TF to fall back on a device                                                                                                           
                    with certain operation implemented                                                                                                           
                    when the proffered device doesn't                                                                                                            
                    exit, not allow all_soft_placement                                                                                                           
                    wold result an error, if                                                                                                                     
                    `log_device_placement' is set.                                                                                                               
                                                                                                                                                                 
                    TF log on which devices CPU/GPU it                                                                                                           
                    place operation. This useful for                                                                                                             
                    debugging.e                                                                                                                                  
                                                                                                                                                                 
 Training           when we instantiate our cnn model     cnn = TextCNN(                                                                                         
                    all the variable and operations       sequence_length=x_train.shape[1],                                                                      
                    defined will be placed inoto the      num_classes=2,                                                                                         
                    default graph and session             vocab_size=len(vocabulary),                                                                            
                                                          embedding_size=FLAGS.embedding_dim,                                                                    
                    Next, we defined how to optimize      filter_sizes=map(int, FLAGS.filter_sizes.split(",")),                                                  
                    our nextwork loss function.           num_filters=FLAGS.num_filters)                                                                         
                                                                                                                                                                 
                    TF has several built-in optimizer.                                                                                                           
                    Here we use ADAM.                                                                                                                            
                                                          global_step = tf.Variable(0, name="global_step", trainable=False)                                      
                    `train_op' here is a newly created    optimizer = tf.train.AdamOptimizer(1e-4)                                                               
                    operation that we can run to          grads_and_vars = optimizer.compute_gradients(cnn.loss)                                                 
                    perfume a gradient update on our      train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)                          
                    parameters.                                                                                                                                  
                                                                                                                                                                 
                    Each execution of `train_op'  is                                                                                                             
                    training setup. TF automatically                                                                                                             
                    figure which variable are                                                                                                                    
                    trainable and calculate their                                                                                                                
                    gradients.                                                                                                                                   
                                                                                                                                                                 
                    By defining `global_setp' variable                                                                                                           
                    passing it to the optimizer we                                                                                                               
                    TF handel the counting of training                                                                                                           
                    setp for us. The global setp will                                                                                                            
                    be automatically incremented by                                                                                                              
                    one every time you execute tain_op.                                                                                                          
                                                                                                                                                                 
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Summaries                                                # Output directory for models and summaries                                                            
                    TF has a concept of summaries,        timestamp = str(int(time.time()))                                                                      
                    which allow you to keep track         out_dir = os.path.abspath(os.path.join(os.path.curdir, "runs", timestamp))                             
                    and visualize various quantities      print("Writing to {}\n".format(out_dir))                                                               
                    during training for evaluation.       # Summaries for loss and accuracy                                                                      
                    For example, you probably want tp     loss_summary = tf.scalar_summary("loss", cnn.loss)                                                     
                    keep track of how your loss and       acc_summary = tf.scalar_summary("accuracy", cnn.accuracy)                                              
                    the accuracy evolve over time.                                                                                                               
                                                                                                                                                                 
                    You can also keep track of more       # Train Summaries                                                                                      
                    complex quantities, such as           train_summary_op = tf.merge_summary([loss_summary, acc_summary])                                       
                    histograms of layer activation.       train_summary_dir = os.path.join(out_dir, "summaries", "train")                                        
                                                          train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)                       
                    Summaries are serialized object                                                                                                              
                                                          # Dev summaries                                                                                        
                    Here, see the code, we are            dev_summary_op = tf.merge_summary([loss_summary, acc_summary])                                         
                    separately keeping track of           dev_summary_dir = os.path.join(out_dir, "summaries", "dev")                                            
                    summaries for training and            dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph_def)                           
                    evaluation.                                                                                                                                  
                                                                                                                                                                 
                    In our case, these are the same                                                                                                              
                    quantities, but you may have                                                                                                                 
                    quantities that you want to track                                                                                                            
                    during training only, such as                                                                                                                
                    parameters update value.                                                                                                                     
                                                                                                                                                                 
                    tf.merge_summary is a convenience                                                                                                            
                    function that merges multiple                                                                                                                
                    summary operation into a single                                                                                                              
                    operation that we can execuate                                                                                                               
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 checking point                                                                                                                                                  
                    Another TF feature you typically                                                                                                             
                    want to use checkpointing - saving    # Checkpointing                                                                                        
                    the parameters of you model to        checkpoint_dir = os.path.abspath(os.path.join(out_dir, "checkpoints"))                                 
                    restore them later on. Checkpoint     checkpoint_prefix = os.path.join(checkpoint_dir, "model")                                              
                    can be used to continue training      # Tensorflow assumes this directory already exists so we need to create it                             
                    at a later point, or to pick          if not os.path.exists(checkpoint_dir):                                                                 
                    the best parameters setting using     os.makedirs(checkpoint_dir)                                                                            
                    early stopping.                       saver = tf.train.Saver(tf.all_variables())                                                             
                                                                                                                                                                 
                                                                                                                                                                 
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Initializing                                                                                                                                                    
 the                Before we train the model we also,    sess.run(tf.initialize_all_variables())                                                                
 variables          need tp initialize the variable in                                                                                                           
                    our graph.                                                                                                                                   
                                                                                                                                                                 
                    The `initialize_all_variable'                                                                                                                
                    function is a convenience function                                                                                                           
                    run all of the initializer.                                                                                                                  
                                                                                                                                                                 
                    we've defined for our variables.                                                                                                             
                    you can also call the initializer                                                                                                            
                    of your variables manually.                                                                                                                  
                                                                                                                                                                 
                    That's useful if you want to                                                                                                                 
                    initialize you embedding with                                                                                                                
                    pre-trained value (check the                                                                                                                 
                    second file) python w2v we                                                                                                                   
                    initialize with 300-dimension.                                                                                                               
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 single                                                   def train_step(x_batch, y_batch):                                                                      
 training           Let's now define function for         """                                                                                                    
 step               single training step, evaluating      A single training step                                                                                 
                    the model on a batch of data and      """                                                                                                    
                    updating the model parameters         feed_dict = {                                                                                          
                                                          cnn.input_x: x_batch,                                                                                  
                                                          cnn.input_y: y_batch,                                                                                  
                    `feed_dict' contains the data for     cnn.dropout_keep_prob: FLAGS.dropout_keep_prob                                                         
                    the placeholder nodes we pass         }                                                                                                      
                    to our network. You must feed value   _, step, summaries, loss, accuracy = sess.run(                                                         
                    for all placeholder nodes, or TF      [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],                                     
                    will throw an error.                  feed_dict)                                                                                             
                                                          time_str = datetime.datetime.now().isoformat()                                                         
                    Another way to work with input data   print("{}: step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))                       
                    using *queuse*                        train_summary_writer.add_summary(summaries, step)                                                      
                                                                                                                                                                 
                    Next, we execute our `train_op'                                                                                                              
                    using `session.run', which return                                                                                                            
                    values for all operation we ask it                                                                                                           
                    to evaluate. Note that `train_op'                                                                                                            
                    return nothing, its just update                                                                                                              
                    parameters of the network.                                                                                                                   
                                                                                                                                                                 
                    Finally we print the loss and                                                                                                                
                    the current training batch and save                                                                                                          
                    the summaries to disk. *Note* that                                                                                                           
                    the loss and accuracy for training                                                                                                           
                    batch may very significantly across                                                                                                          
                    batches if you batch size is small                                                                                                           
                                                                                                                                                                 
                    So, we write a similar function       def dev_setp(x_batch, y_batch, writer=None):                                                           
                    to evaluate the loss and accuracy                                                                                                            
                    on a arbitrary data set, such         feed_dict ={                                                                                           
                    as validation set or the whole        cnn.input_x: x_batch                                                                                   
                    training set. Essentially this        cnn.input_y: y_batch                                                                                   
                    function does same as the above,      cnn.drop_keep_prob: 1.0                                                                                
                    but without the training operation.   }                                                                                                      
                    also *disable dropout*                                                                                                                       
                                                          step, summaries, loss, accuary = sess.run(                                                             
                                                          [global, step, dev_summary_op, cnn.loss, cnn.accuary],                                                 
                                                          feed_dict)                                                                                             
                                                          time_str = datetime.datetime.now().isoformat()                                                         
                                                          print('{}: setp{}, loss{:g}, acc{:g}. format{time_str, step, loss, accuracy))                          
                                                          if writer:                                                                                             
                                                          writer.add.summary(summaries,step)                                                                     
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
 Training loop                                                                                                                                                   
                    We iterate over batches of our        Generate batches                                                                                       
                    data, call the `train_setp'           batches = data_helpers.batch_iter(                                                                     
                    function for each batch, and          zip(x_train, y_train), FLAGS.batch_size, FLAGS.num_epochs)                                             
                    occasionally evaluate and             # Training loop. For each batch…                                                                       
                    checkpoint our model.                 for batch in batches:                                                                                  
                                                          x_batch, y_batch = zip(*batch)                                                                         
                                                          train_step(x_batch, y_batch)                                                                           
                    Here, code `batch_iter' is helper     current_step = tf.train.global_step(sess, global_step)                                                 
                    function to batch the data            if current_step % FLAGS.evaluate_every == 0:                                                           
                    tf.train.global_setp is               print("\nEvaluation:")                                                                                 
                    convenience function that resturns    dev_step(x_dev, y_dev, writer=dev_summary_writer)                                                      
                    the value of `global_setup'           print("")                                                                                              
                                                          if current_step % FLAGS.checkpoint_every == 0:                                                         
                                                          path = saver.save(sess, checkpoint_prefix, global_step=current_step)                                   
                                                          print("Saved model checkpoint to {}\n".format(path))                                                   
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━



